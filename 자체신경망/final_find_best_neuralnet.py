import numpy as np
import matplotlib.pyplot as plt
import random # â­ï¸ random ì„í¬íŠ¸
import time   # â­ï¸ time ì„í¬íŠ¸
from mnist_reader import load_mnist
from multi_layer_net import MultiLayerNet 
from collections import OrderedDict

# 1. ë°ì´í„° ë¡œë“œ
print("ë°ì´í„° ë¡œë”© ì¤‘...")
x_train_all, t_train_all = load_mnist('data/fashion', kind='train')
x_test, t_test = load_mnist('data/fashion', kind='t10k')

# 2. ì •ê·œí™”
x_train_all = x_train_all / 255.0
x_test = x_test / 255.0 

# 3. í›ˆë ¨ / ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
validation_rate = 0.20
validation_num = int(x_train_all.shape[0] * validation_rate) 
shuffle_mask = np.random.permutation(x_train_all.shape[0])
x_train_all = x_train_all[shuffle_mask]
t_train_all = t_train_all[shuffle_mask]
x_val = x_train_all[:validation_num]
t_val = t_train_all[:validation_num]
x_train = x_train_all[validation_num:] 
t_train = t_train_all[validation_num:] 

print(f"ë°ì´í„° ë¶„í•  ì™„ë£Œ: Train({x_train.shape[0]}), Validation({x_val.shape[0]})")

# -----------------------------------------------------------------
# ğŸ“Œ 4. ê¸°ë³¸ í•™ìŠµ ì„¤ì • (íƒìƒ‰ìš©)
# -----------------------------------------------------------------
# â­ï¸ íƒìƒ‰ ì‹œì—ëŠ” Epochë¥¼ ì¤„ì—¬ì„œ (ì˜ˆ: 5 Epoch) ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
# 1 Epoch = 480 iter (48000 / 100)
iters_num_search = 10000 # ( 1íšŒ ì‹œë„ : 5 Epochs = 2400) 
batch_size = 100
train_size = x_train.shape[0]
iter_per_epoch = max(train_size / batch_size, 1)

def train_model(lr, hidden_size):
    """ì§€ì •ëœ lr, hsë¡œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ìµœê³  ê²€ì¦ ì •í™•ë„ë¥¼ ë°˜í™˜"""
    hidden_list = [hidden_size] # 2ì¸µ ë„¤íŠ¸ì›Œí¬ë¡œ ê³ ì •
    network = MultiLayerNet(input_size=784, 
                            hidden_size_list=hidden_list, 
                            output_size=10)
    
    max_val_acc_trial = 0.0 # í•´ë‹¹ í…ŒìŠ¤íŠ¸ì˜ ìµœê³  ì •í™•ë„

    for i in range(1, iters_num_search + 1):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]

        grad = network.gradient(x_batch, t_batch)

        for param_key in network.params.keys():
            if param_key.startswith('W') or param_key.startswith('b'):
                network.params[param_key] -= lr * grad[param_key]

        if i % iter_per_epoch == 0:
            val_acc = network.accuracy(x_val, t_val)
            max_val_acc_trial = max(max_val_acc_trial, val_acc)
            
    return max_val_acc_trial

# -----------------------------------------------------------------
# 5. 1ë‹¨ê³„: íƒìƒ‰ (Wide Random Search)
# -----------------------------------------------------------------
print("\n========== 1ë‹¨ê³„: ë„“ì€ íƒìƒ‰ (Random Search) ì‹œì‘ ==========")
num_trials_s1 = 30 # ğŸ“Œ 1ë‹¨ê³„ íƒìƒ‰ íšŸìˆ˜ (30~100 ì¶”ì²œ)

# â­ï¸ ë„“ì€ íƒìƒ‰ ë²”ìœ„
lr_range_wide = (-4, -0.3) # (log) 10^-4 (0.0001) ~ 10^-0.3 (0.5)
hs_range_wide = (20, 150)  # (int) 20 ~ 150
results_s1 = [] # (val_acc, lr, hs) ì €ì¥

start_time_s1 = time.time()

for i in range(num_trials_s1):
    # â­ï¸ ë²”ìœ„ ì•ˆì—ì„œ ë§¤ë²ˆ ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„° ë¬´ì‘ìœ„ ì¶”ì¶œ
    lr = 10**np.random.uniform(lr_range_wide[0], lr_range_wide[1])
    hs = np.random.randint(hs_range_wide[0], hs_range_wide[1])
    
    key = f"[S1 {i+1}/{num_trials_s1}] lr={lr:.6f} hs={hs}"
    print(f"--- STARTING TEST: {key} ---")
    
    # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    val_acc = train_model(lr, hs)
    
    print(f"--- TEST FINISHED: {key} | Max Acc: {val_acc:.4f} ---")
    results_s1.append((val_acc, lr, hs))

end_time_s1 = time.time()
print(f"1ë‹¨ê³„ íƒìƒ‰ ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {end_time_s1 - start_time_s1:.2f}ì´ˆ)")


# -----------------------------------------------------------------
# 6. 2ë‹¨ê³„: ìµœì  ë²”ìœ„ ë„ì¶œ
# -----------------------------------------------------------------
print("\n========== 2ë‹¨ê³„: ìµœì  ë²”ìœ„ ë„ì¶œ ==========")

# ì •í™•ë„(val_acc) ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
results_s1.sort(key=lambda x: x[0], reverse=True)

best_s1_result = results_s1[0]
print(f"ğŸ¥‡ 1ë‹¨ê³„ ìµœê³  ì •í™•ë„: {best_s1_result[0]:.4f} (lr={best_s1_result[1]:.6f}, hs={best_s1_result[2]})")

# 1ë‹¨ê³„ ìƒìœ„ 20%ì˜ ê²°ê³¼ë¥¼ "ëŒ€ë°•" ì¡°í•©ìœ¼ë¡œ ê°„ì£¼
top_n_percent = 0.20
top_n_count = int(num_trials_s1 * top_n_percent)
if top_n_count < 3: top_n_count = max(3, len(results_s1)) # ìµœì†Œ 3ê°œ
top_results = results_s1[:top_n_count]

# "ëŒ€ë°•" ì¡°í•©ë“¤ì˜ min/max ë²”ìœ„ë¥¼ ì°¾ìŒ
min_lr = min(res[1] for res in top_results)
max_lr = max(res[1] for res in top_results)
min_hs = min(res[2] for res in top_results)
max_hs = max(res[2] for res in top_results)

print(f"--- 2ë‹¨ê³„ ì •ë°€ íƒìƒ‰ ë²”ìœ„ (ìƒìœ„ {top_n_percent*100}%) ---")
print(f"LR ë²”ìœ„: {min_lr:.6f} ~ {max_lr:.6f}")
print(f"HS ë²”ìœ„: {min_hs} ~ {max_hs}")


# -----------------------------------------------------------------
# 7. 3ë‹¨ê³„: ì •ë°€ íƒìƒ‰ (Narrow Grid Search)
# -----------------------------------------------------------------
print("\n========== 3ë‹¨ê³„: ì •ë°€ íƒìƒ‰ (Grid Search) ì‹œì‘ ==========")
steps_s2 = 5 # ğŸ“Œ ì¢ì€ ë²”ìœ„ì—ì„œ 5x5 = 25íšŒ ì´˜ì´˜í•˜ê²Œ íƒìƒ‰
lr_range_narrow = np.linspace(min_lr, max_lr, steps_s2)
hs_range_narrow = np.linspace(min_hs, max_hs, steps_s2, dtype=int)

best_val_acc_s2 = 0.0
best_params_s2 = {}

start_time_s2 = time.time()
trial_count = 0

for lr in lr_range_narrow:
    for hs in hs_range_narrow:
        trial_count += 1
        key = f"[S2 {trial_count}/{steps_s2**2}] lr={lr:.6f} hs={hs}"
        print(f"--- STARTING TEST: {key} ---")

        val_acc = train_model(lr, hs)
        print(f"--- TEST FINISHED: {key} | Max Acc: {val_acc:.4f} ---")
        
        if val_acc > best_val_acc_s2:
            best_val_acc_s2 = val_acc
            best_params_s2 = {'learning_rate': lr, 'hidden_size': hs}

end_time_s2 = time.time()
print(f"3ë‹¨ê³„ ì •ë°€ íƒìƒ‰ ì™„ë£Œ. (ì†Œìš” ì‹œê°„: {end_time_s2 - start_time_s2:.2f}ì´ˆ)")


# -----------------------------------------------------------------
# 8. ìµœì¢… ê²°ê³¼ ë°œí‘œ
# -----------------------------------------------------------------
print("\n========== ALL TESTS FINISHED ==========")
print(f"ğŸ¥‡ 1ë‹¨ê³„ ìµœê³  ì •í™•ë„ (ëœë¤): {best_s1_result[0]:.4f}")
print(f"ğŸ¥‡ 3ë‹¨ê³„ ìµœê³  ì •í™•ë„ (ì •ë°€): {best_val_acc_s2:.4f}")
print(f"ğŸ¥‡ ìµœì¢… ìµœì  íŒŒë¼ë¯¸í„°: {best_params_s2}")


# -----------------------------------------------------------------
# 9. (ë³´ë„ˆìŠ¤) 1ë‹¨ê³„ 'ì§€ì§„íŒŒ' ê·¸ë˜í”„ ì‹œê°í™”
# -----------------------------------------------------------------
print("1ë‹¨ê³„ íƒìƒ‰(ì§€ì§„íŒŒ) ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤...")
lrs = [res[1] for res in results_s1]
accs = [res[0] for res in results_s1]
hss = [res[2] for res in results_s1] # ì ì˜ í¬ê¸°(size)ë¡œ ì‚¬ìš©

plt.figure(figsize=(10, 6))
# x=í•™ìŠµë¥ , y=ì •í™•ë„, s=ì€ë‹‰ì¸µí¬ê¸°, c=ì •í™•ë„, alpha=íˆ¬ëª…ë„
plt.scatter(lrs, accs, s=hss, c=accs, cmap='viridis', alpha=0.7)
plt.xscale('log') # â­ï¸ Xì¶•ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ
plt.xlabel('Learning Rate (Log Scale)')
plt.ylabel('Validation Accuracy')
plt.title(f"Stage 1: Random Search Results ({num_trials_s1} trials)")
plt.colorbar(label='Accuracy')
plt.grid(True, which="both", ls="--")

# 2ë‹¨ê³„ì—ì„œ ì°¾ì€ "ëŒ€ë°•" ë²”ìœ„(ë…¹ìƒ‰)ì™€ ìµœì¢… ìµœê³ ì (ë¹¨ê°„ìƒ‰) í‘œì‹œ
plt.axvspan(min_lr, max_lr, color='green', alpha=0.1, label=f'Best Range (Top {top_n_percent*100}%)')
plt.scatter(best_s1_result[1], best_s1_result[0], 
            s=best_s1_result[2], color='red', 
            edgecolors='black', zorder=5, label='S1 Best')
plt.legend()
plt.show()